{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\\Bible\\t_asv.csv\n",
      "./data\\Bible\\t_bbe.csv\n",
      "./data\\Bible\\t_dby.csv\n",
      "./data\\Bible\\t_kjv.csv\n",
      "./data\\Bible\\t_wbt.csv\n",
      "./data\\Bible\\t_web.csv\n",
      "./data\\Bible\\t_ylt.csv\n",
      "./data\\fasttext-wikinews\\wiki-news-300d-1M.vec\n",
      "./data\\fasttext-wikinews\\wiki-news-300d-1M.vec.pt\n",
      "./data\\preprocessed\\bibles_suffled.csv\n",
      "./data\\preprocessed\\newbible.tsv\n",
      "./data\\preprocessed\\oldbible.tsv\n",
      "./data\\preprocessed\\test_df.csv\n",
      "./data\\preprocessed\\train_df.csv\n",
      "./data\\preprocessed\\val_df.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import codecs\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "        \n",
    "import torch as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "CUDA = tf.cuda.is_available()\n",
    "DEVICE=tf.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "               decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                            src_mask, trg_mask, hidden=decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # we need to manually concatenate the final states for both directions\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
    "\n",
    "        return output, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                          hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "\n",
    "        # compute context vector using attention mechanism\n",
    "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
    "        context, attn_probs = self.attention(\n",
    "            query=query, proj_key=proj_key,\n",
    "            value=encoder_hidden, mask=src_mask)\n",
    "\n",
    "        # update rnn hidden state\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "\n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
    "                src_mask, trg_mask, hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "                                         \n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "        \n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "        \n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "        \n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        # We first project the query (the decoder state).\n",
    "        # The projected keys (the encoder states) were already pre-computated.\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate scores.\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Turn scores to probabilities.\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas        \n",
    "        \n",
    "        # The context vector is the weighted sum of the values.\n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
    "        return context, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model.cuda() if USE_CUDA else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        \n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                           batch.src_mask, batch.trg_mask,\n",
    "                                           batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(example_iter, model, n=2, max_len=100, \n",
    "                   sos_index=1, \n",
    "                   src_eos_index=None, \n",
    "                   trg_eos_index=None, \n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        src_eos_index = src_vocab.stoi[SOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "        \n",
    "    for i, batch in enumerate(example_iter):\n",
    "        if(i % 2 == 0):\n",
    "            continue\n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
    "      \n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (count+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(num_words=11, batch_size=16, num_batches=100, length=10, pad_index=0, sos_index=1):\n",
    "    \"\"\"Generate random data for a src-tgt copy task.\"\"\"\n",
    "    for i in range(num_batches):\n",
    "        data = torch.from_numpy(\n",
    "          np.random.randint(1, num_words, size=(batch_size, length)))\n",
    "        data[:, 0] = sos_index\n",
    "        data = data.cuda() if USE_CUDA else data\n",
    "        src = data[:, 1:]\n",
    "        trg = data\n",
    "        src_lengths = [length-1] * batch_size\n",
    "        trg_lengths = [length] * batch_size\n",
    "        yield Batch((src, src_lengths), (trg, trg_lengths), pad_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "    \n",
    "    output = np.array(output)\n",
    "        \n",
    "    # cut off everything starting from </s> \n",
    "    # (only when eos_index provided)\n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]      \n",
    "    \n",
    "    return output, np.concatenate(attention_scores, axis=1)\n",
    "  \n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "from torchtext import vocab\n",
    "\n",
    "import en_core_web_sm\n",
    "en = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the path to the localy saved vectors\n",
    "vec = vocab.Vectors('wiki-news-300d-1M.vec', './data/fasttext-wikinews/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"    \n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "LOWER = True\n",
    "\n",
    "# we include lengths to provide to the RNNs\n",
    "SRC = data.Field(tokenize=tokenize_en, \n",
    "                 batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                 unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "TRG = data.Field(tokenize=tokenize_en, \n",
    "                 batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                 unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "MAX_LEN_SRC = 50 \n",
    "MAX_LEN_TRG = 50  \n",
    "\n",
    "fields = [\n",
    "    ('ID', None), # we dont need this, so no processing\n",
    "    ('src', SRC), # process it as label\n",
    "    ('trg', TRG) # process it as text\n",
    "]\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(path='./data/preprocessed/',train='train_df.csv',\n",
    "                                    validation='val_df.csv', test='test_df.csv', format='csv',\n",
    "                                    fields=fields,\n",
    "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN_SRC \n",
    "                                    and len(vars(x)['trg']) <= MAX_LEN_TRG)\n",
    "\n",
    "MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "\n",
    "SRC.build_vocab(train_data.src,valid_data.src, min_freq=MIN_FREQ, vectors=vec)\n",
    "TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ, vectors=vec)\n",
    "PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes (number of sentence pairs):\n",
      "train 21149\n",
      "valid 6064\n",
      "test 3036 \n",
      "\n",
      "First training example:\n",
      "src: then from before him sent is the extremity of the hand and the writing is noted down\n",
      "trg: then was the part of the hand sent from before him and this writing was inscribed \n",
      "\n",
      "Most common words (src):\n",
      "       and      49636\n",
      "       the      46363\n",
      "        of      30955\n",
      "        to      18368\n",
      "        in      12388\n",
      "        is      10530\n",
      "        he       8418\n",
      "       for       7816\n",
      "       his       7043\n",
      "      hath       6949 \n",
      "\n",
      "Most common words (trg):\n",
      "       the      37554\n",
      "       and      23022\n",
      "        of      22092\n",
      "        to      13885\n",
      "       you      10015\n",
      "        in       8335\n",
      "        he       6941\n",
      "       for       5953\n",
      "      that       5573\n",
      "       his       5361 \n",
      "\n",
      "First 10 words (src):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 and\n",
      "05 the\n",
      "06 of\n",
      "07 to\n",
      "08 in\n",
      "09 is \n",
      "\n",
      "First 10 words (trg):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 the\n",
      "05 and\n",
      "06 of\n",
      "07 to\n",
      "08 you\n",
      "09 in \n",
      "\n",
      "Number of text words (types): 4878\n",
      "Number of title words (types): 4351 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    print(\"Data set sizes (number of sentence pairs):\")\n",
    "    print('train', len(train_data))\n",
    "    print('valid', len(valid_data))\n",
    "    print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
    "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
    "    \n",
    "    print(\"Most common words (src):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    print(\"Most common words (trg):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    print(\"First 10 words (src):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    print(\"First 10 words (trg):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of text words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of title words (types):\", len(trg_field.vocab), \"\\n\")\n",
    "    \n",
    "    \n",
    "print_data_info(train_data, valid_data, test_data,   SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(train_data, batch_size=batch_size, train=True, \n",
    "                                 sort_within_batch=True, \n",
    "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
    "                                 device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False, \n",
    "                           device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebatch(pad_idx, batch):\n",
    "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
    "    return Batch(batch.src, batch.trg, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=50, lr=0.0003, print_every=100):\n",
    "    \"\"\"Train a model on IWSLT\"\"\"\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    # optionally add label smoothing; see the Annotated Transformer\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    dev_perplexities = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      \n",
    "        print(\"Epoch\", epoch)\n",
    "        torch.save(model.state_dict(), 'model_epoch_' + str(epoch))\n",
    "\n",
    "        model.train()\n",
    "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
    "                                     model,\n",
    "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
    "                                     print_every=print_every)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
    "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
    "\n",
    "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
    "                                       model, \n",
    "                                       SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
    "            dev_perplexities.append(dev_perplexity)\n",
    "    torch.save(model.state_dict(), 'model_Final')\n",
    "\n",
    "    return dev_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 100 Loss: 140.178543 Tokens per Sec: 4334.535736\n",
      "Epoch Step: 200 Loss: 190.616898 Tokens per Sec: 4478.961417\n",
      "Epoch Step: 300 Loss: 116.483017 Tokens per Sec: 4501.380064\n",
      "Epoch Step: 400 Loss: 109.755287 Tokens per Sec: 4382.248448\n",
      "Epoch Step: 500 Loss: 165.891815 Tokens per Sec: 4301.038192\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  he has his disciples in his hand and the king and the children and they shall be all israel and you shall be you and you shall be <unk> and to you\n",
      "\n",
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  therefore from him from him and the king and him him\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who are not be <unk> of the children of man of god of god\n",
      "\n",
      "Validation perplexity: 55.696016\n",
      "Epoch 1\n",
      "Epoch Step: 100 Loss: 117.520958 Tokens per Sec: 4566.925363\n",
      "Epoch Step: 200 Loss: 167.434982 Tokens per Sec: 4501.008301\n",
      "Epoch Step: 300 Loss: 63.296467 Tokens per Sec: 4524.531735\n",
      "Epoch Step: 400 Loss: 75.960663 Tokens per Sec: 4526.108026\n",
      "Epoch Step: 500 Loss: 96.135597 Tokens per Sec: 4397.547476\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  he has his right hand in his hand and the levites and yahweh made an offering of israel and you shall be and you shall be holy and give you shall give you shall give you for an altar to yahweh\n",
      "\n",
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  therefore from him and the <unk> and <unk> him and <unk> the <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who don eat of the flesh of the flesh of the flesh of the flesh of the man and of god and the father of god\n",
      "\n",
      "Validation perplexity: 27.313027\n",
      "Epoch 2\n",
      "Epoch Step: 100 Loss: 71.285820 Tokens per Sec: 4406.553938\n",
      "Epoch Step: 200 Loss: 38.663620 Tokens per Sec: 4460.371204\n",
      "Epoch Step: 300 Loss: 58.115078 Tokens per Sec: 4493.352837\n",
      "Epoch Step: 400 Loss: 84.338753 Tokens per Sec: 4391.285123\n",
      "Epoch Step: 500 Loss: 57.578533 Tokens per Sec: 4508.051644\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  he brought his life in his hand and struck the days and yahweh made great salvation for all israel you have seen and will make you and give you sin offering to kill them\n",
      "\n",
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  therefore therefore from him the whole and gave him <unk>\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who doesn eat of blood neither of bread nor of bread but of god but god\n",
      "\n",
      "Validation perplexity: 17.826554\n",
      "Epoch 3\n",
      "Epoch Step: 100 Loss: 73.590767 Tokens per Sec: 4346.596723\n",
      "Epoch Step: 200 Loss: 77.371178 Tokens per Sec: 4400.870953\n",
      "Epoch Step: 300 Loss: 63.287373 Tokens per Sec: 4327.018621\n",
      "Epoch Step: 400 Loss: 59.187405 Tokens per Sec: 4367.025055\n",
      "Epoch Step: 500 Loss: 84.316238 Tokens per Sec: 4367.486002\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  he put his life in his hand and struck the philistine and yahweh made great salvation for all israel you have seen and will make yourselves and make yourselves against blood to make atonement to kill david to kill david\n",
      "\n",
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  take therefore him from him and gave him <unk> him\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who doesn eat of blood neither shall not eat of the flesh nor of men but of god\n",
      "\n",
      "Validation perplexity: 13.531113\n",
      "Epoch 4\n",
      "Epoch Step: 100 Loss: 57.376881 Tokens per Sec: 4361.218539\n",
      "Epoch Step: 200 Loss: 67.530220 Tokens per Sec: 4382.392827\n",
      "Epoch Step: 300 Loss: 66.925163 Tokens per Sec: 4395.152799\n",
      "Epoch Step: 400 Loss: 60.444275 Tokens per Sec: 4321.580254\n",
      "Epoch Step: 500 Loss: 41.386929 Tokens per Sec: 4350.594008\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  he put his life in his hand and struck the philistine and yahweh made great salvation for all israel you have seen and will rejoice and do you have sinned against my blood to kill david\n",
      "\n",
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  therefore therefore from him the sluggard and gave him <unk> ten ten\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who doesn eat of blood neither of the flesh of the flesh of the body of god but of god were born\n",
      "\n",
      "Validation perplexity: 10.912111\n",
      "Epoch 5\n",
      "Epoch Step: 100 Loss: 54.114120 Tokens per Sec: 4445.034013\n",
      "Epoch Step: 200 Loss: 55.748253 Tokens per Sec: 4348.172758\n",
      "Epoch Step: 300 Loss: 64.587471 Tokens per Sec: 4363.610300\n",
      "Epoch Step: 400 Loss: 31.476610 Tokens per Sec: 4453.415391\n",
      "Epoch Step: 500 Loss: 78.841888 Tokens per Sec: 4268.246030\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  and he put his life in his hand and struck the philistine and yahweh made great salvation for all israel you have seen and all that you have seen and do you do and do you do evil against me blood to kill david for nothing\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  therefore take him from him the truth and gave him the ten talents\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who doesn eat of blood nor of flesh nor of man but of god were born\n",
      "\n",
      "Validation perplexity: 10.560108\n",
      "Epoch 6\n",
      "Epoch Step: 100 Loss: 73.641403 Tokens per Sec: 4235.731805\n",
      "Epoch Step: 200 Loss: 32.476990 Tokens per Sec: 4274.279104\n",
      "Epoch Step: 300 Loss: 38.626911 Tokens per Sec: 4236.828523\n",
      "Epoch Step: 400 Loss: 65.294235 Tokens per Sec: 4390.729857\n",
      "Epoch Step: 500 Loss: 51.467209 Tokens per Sec: 4252.710579\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  he put his life in his hand and struck the philistine and yahweh made great salvation for all israel you have seen and do and do you do sin against innocent blood to kill david for these things\n",
      "\n",
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  take therefore from him the <unk> and gave him ten ten talents\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who doesn eat of blood neither of the flesh nor of men but of god were born\n",
      "\n",
      "Validation perplexity: 8.996875\n",
      "Epoch 7\n",
      "Epoch Step: 100 Loss: 36.645844 Tokens per Sec: 4319.673639\n",
      "Epoch Step: 200 Loss: 55.203358 Tokens per Sec: 4408.541782\n",
      "Epoch Step: 300 Loss: 35.596394 Tokens per Sec: 4438.465652\n",
      "Epoch Step: 400 Loss: 46.574265 Tokens per Sec: 4359.087562\n",
      "Epoch Step: 500 Loss: 51.905170 Tokens per Sec: 4324.101042\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  yes he put his life in his hand and struck the philistine and yahweh made great salvation for all israel you have forsaken and do you do so that they have sinned against me blood to kill david\n",
      "\n",
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  therefore take away from him the posts and gave him the ten talents\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who doesn regard of blood neither of the flesh nor of the man but of god were born\n",
      "\n",
      "Validation perplexity: 8.669501\n",
      "Epoch 8\n",
      "Epoch Step: 100 Loss: 24.817148 Tokens per Sec: 4223.145019\n",
      "Epoch Step: 200 Loss: 48.336292 Tokens per Sec: 4186.959982\n",
      "Epoch Step: 300 Loss: 47.886734 Tokens per Sec: 4080.741606\n",
      "Epoch Step: 400 Loss: 37.925499 Tokens per Sec: 4341.858984\n",
      "Epoch Step: 500 Loss: 12.112428 Tokens per Sec: 4337.140697\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  yes he put his life in his hand and struck the philistine and yahweh made great salvation for all israel you have seen and have dreamed and why do you sin against me blood to kill david to be avenged to death\n",
      "\n",
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  take therefore from him the whole spiritual and gave him <unk> ten ten talents\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who doesn eat of blood neither of the flesh nor of man but of god were born\n",
      "\n",
      "Validation perplexity: 8.504696\n",
      "Epoch 9\n",
      "Epoch Step: 100 Loss: 24.586576 Tokens per Sec: 4404.470574\n",
      "Epoch Step: 200 Loss: 56.878551 Tokens per Sec: 4485.098915\n",
      "Epoch Step: 300 Loss: 39.652092 Tokens per Sec: 4465.223129\n",
      "Epoch Step: 400 Loss: 30.657780 Tokens per Sec: 4437.630562\n",
      "Epoch Step: 500 Loss: 54.229401 Tokens per Sec: 4449.584625\n",
      "\n",
      "Example #1\n",
      "Src :  <s> yea he putteth his life in his hand and smiteth the philistine and jehovah worketh great salvation for all israel thou hast seen and dost rejoice and why dost thou sin against innocent blood to put david to death for nought\n",
      "Trg :  for he put his life in his hand and struck the philistine and yahweh worked great victory for all israel you saw it and did rejoice why then will you sin against innocent blood to kill david without cause\n",
      "Pred:  yes he put his life in his hand and struck the philistine and yahweh made great victory for all israel you have seen and do you do so why do you sin against me blood to kill david for they\n",
      "\n",
      "Example #2\n",
      "Src :  <s> take therefore from him the talent and give to him having the ten talents\n",
      "Trg :  take away therefore the talent from him and give it to him who has the ten talents\n",
      "Pred:  therefore take away from him the whole and gave him ten ten talents\n",
      "\n",
      "Example #3\n",
      "Src :  <s> who not of blood nor of will of flesh nor of will of man but of god were begotten\n",
      "Trg :  who were born not of blood nor of the will of the flesh nor of the will of man but of god\n",
      "Pred:  who doesn regard of blood nor of flesh nor of men but of god were born\n",
      "\n",
      "Validation perplexity: 8.097168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): GRU(256, 256, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): BahdanauAttention(\n",
       "      (key_layer): Linear(in_features=512, out_features=256, bias=False)\n",
       "      (query_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (energy_layer): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "    (rnn): GRU(768, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
       "    (bridge): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "    (pre_output_layer): Linear(in_features=1024, out_features=256, bias=False)\n",
       "  )\n",
       "  (src_embed): Embedding(4878, 256)\n",
       "  (trg_embed): Embedding(4351, 256)\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=256, out_features=4351, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
    "                   emb_size=256, hidden_size=256,\n",
    "                   num_layers=3, dropout=0.2)\n",
    "\n",
    "dev_perplexities = train(model,num_epochs=10, print_every=100)\n",
    "#model.load_state_dict(torch.load('model_epoch_30'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
