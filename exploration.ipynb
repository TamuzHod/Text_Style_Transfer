{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\\Bible\\t_asv.csv\n",
      "./data\\Bible\\t_bbe.csv\n",
      "./data\\Bible\\t_dby.csv\n",
      "./data\\Bible\\t_kjv.csv\n",
      "./data\\Bible\\t_wbt.csv\n",
      "./data\\Bible\\t_web.csv\n",
      "./data\\Bible\\t_ylt.csv\n",
      "./data\\fasttext-wikinews\\wiki-news-300d-1M.vec\n",
      "./data\\preprocessed\\bibles_suffled.csv\n",
      "./data\\preprocessed\\newbible.tsv\n",
      "./data\\preprocessed\\oldbible.tsv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import codecs\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "        \n",
    "import torch as tf\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>v</th>\n",
       "      <th>new_bib</th>\n",
       "      <th>old_bib</th>\n",
       "      <th>literal_bib</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24007008</th>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>behold, you trust in lying words, that can't p...</td>\n",
       "      <td>behold, ye trust in lying words, that cannot p...</td>\n",
       "      <td>lo, ye are trusting for yourselves on the word...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18026001</th>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>then job answered,</td>\n",
       "      <td>but job answered and said,</td>\n",
       "      <td>and job answereth and saith: --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6019015</th>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>and kattath, and nahalal, and shimron, and ida...</td>\n",
       "      <td>and kattath, and nahallal, and shimron, and id...</td>\n",
       "      <td>and kattath, and nahallal, and shimron, and id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41014056</th>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>for many gave false testimony against him, and...</td>\n",
       "      <td>for many bare false witness against him, but t...</td>\n",
       "      <td>for many were bearing false testimony against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24026009</th>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>why have you prophesied in the name of yahweh,...</td>\n",
       "      <td>why hast thou prophesied in the name of the lo...</td>\n",
       "      <td>wherefore hast thou prophesied in the name of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52005017</th>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>pray without ceasing.</td>\n",
       "      <td>pray without ceasing.</td>\n",
       "      <td>continually pray ye;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43007001</th>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>after these things, jesus was walking in galil...</td>\n",
       "      <td>after these things jesus walked in galilee: fo...</td>\n",
       "      <td>and jesus was walking after these things in ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62005002</th>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>by this we know that we love the children of g...</td>\n",
       "      <td>by this we know that we love the children of g...</td>\n",
       "      <td>in this we know that we love the children of g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19115001</th>\n",
       "      <td>19</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>not to us, yahweh, not to us, but to your name...</td>\n",
       "      <td>not unto us, o lord, not unto us, but unto thy...</td>\n",
       "      <td>not to us, o jehovah, not to us, but to thy na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011025</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>yahweh came down in the cloud, and spoke to hi...</td>\n",
       "      <td>and the lord came down in a cloud, and spake u...</td>\n",
       "      <td>and jehovah cometh down in the cloud, and spea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31096 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           b    c   v                                            new_bib  \\\n",
       "id                                                                         \n",
       "24007008  24    7   8  behold, you trust in lying words, that can't p...   \n",
       "18026001  18   26   1                                 then job answered,   \n",
       "6019015    6   19  15  and kattath, and nahalal, and shimron, and ida...   \n",
       "41014056  41   14  56  for many gave false testimony against him, and...   \n",
       "24026009  24   26   9  why have you prophesied in the name of yahweh,...   \n",
       "...       ..  ...  ..                                                ...   \n",
       "52005017  52    5  17                              pray without ceasing.   \n",
       "43007001  43    7   1  after these things, jesus was walking in galil...   \n",
       "62005002  62    5   2  by this we know that we love the children of g...   \n",
       "19115001  19  115   1  not to us, yahweh, not to us, but to your name...   \n",
       "4011025    4   11  25  yahweh came down in the cloud, and spoke to hi...   \n",
       "\n",
       "                                                    old_bib  \\\n",
       "id                                                            \n",
       "24007008  behold, ye trust in lying words, that cannot p...   \n",
       "18026001                         but job answered and said,   \n",
       "6019015   and kattath, and nahallal, and shimron, and id...   \n",
       "41014056  for many bare false witness against him, but t...   \n",
       "24026009  why hast thou prophesied in the name of the lo...   \n",
       "...                                                     ...   \n",
       "52005017                              pray without ceasing.   \n",
       "43007001  after these things jesus walked in galilee: fo...   \n",
       "62005002  by this we know that we love the children of g...   \n",
       "19115001  not unto us, o lord, not unto us, but unto thy...   \n",
       "4011025   and the lord came down in a cloud, and spake u...   \n",
       "\n",
       "                                                literal_bib  \n",
       "id                                                           \n",
       "24007008  lo, ye are trusting for yourselves on the word...  \n",
       "18026001                    and job answereth and saith: --  \n",
       "6019015   and kattath, and nahallal, and shimron, and id...  \n",
       "41014056  for many were bearing false testimony against ...  \n",
       "24026009  wherefore hast thou prophesied in the name of ...  \n",
       "...                                                     ...  \n",
       "52005017                               continually pray ye;  \n",
       "43007001  and jesus was walking after these things in ga...  \n",
       "62005002  in this we know that we love the children of g...  \n",
       "19115001  not to us, o jehovah, not to us, but to thy na...  \n",
       "4011025   and jehovah cometh down in the cloud, and spea...  \n",
       "\n",
       "[31096 rows x 6 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bibles = pd.read_csv(\"./data/preprocessed/bibles_suffled.csv\", index_col = [0])\n",
    "bibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21767 6219 3110 sum: 31096\n"
     ]
    }
   ],
   "source": [
    "train = round(31096 * 0.7)\n",
    "val = round(31096 * 0.2)\n",
    "test = round(31096 * 0.1)\n",
    "print(train, val, test, 'sum:',train+test+val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "6003015     and when those who bore the ark were come to t...\n",
       "19089048    what man is he who shall live and not see deat...\n",
       "46009006    or have only barnabas and i no right to not work?\n",
       "11004006    and ahishar was over the household; and adonir...\n",
       "1022023     bethuel became the father of rebekah. these ei...\n",
       "                                  ...                        \n",
       "9015008     he took agag the king of the amalekites alive,...\n",
       "10022004    i will call on yahweh, who is worthy to be pra...\n",
       "19037032    the wicked watches the righteous, and seeks to...\n",
       "44002042    they continued steadfastly in the apostles' te...\n",
       "24010021    for the shepherds are become brutish, and have...\n",
       "Name: new_bib, Length: 6219, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bibles.new_bib[train:val+train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "    '''Hyperparameters'''\n",
    "    # data\n",
    "    source_train = bibles.new_bib[0:train]\n",
    "    target_train = bibles.literal_bib[0:train]\n",
    "    source_val = bibles.new_bib[train:val+train]\n",
    "    target_val = bibles.literal_bib[train:val+train]\n",
    "    source_test = bibles.new_bib[val+train:test+val+train]\n",
    "    target_test = bibles.literal_bib[val+train:test+val+train]\n",
    "    \n",
    "    # training\n",
    "    batch_size = 32 # alias = N\n",
    "    lr = 0.0001 # learning rate. In paper, learning rate is adjusted to the global step.\n",
    "    logdir = \"./logdir\" # log directory\n",
    "    \n",
    "    # model\n",
    "    maxlen = 50 # Maximum number of words in a sentence. alias = T.\n",
    "                # Feel free to increase this if you are ambitious.\n",
    "    min_cnt = 5 # words whose occurred less than min_cnt are encoded as <UNK>.\n",
    "    hidden_units = 300 # alias = C\n",
    "    num_epochs = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(fname, text):\n",
    "    '''Constructs vocabulary.\n",
    "    \n",
    "    Args:\n",
    "      fpath: A string. Input file path.\n",
    "      fname: A string. Output file name.\n",
    "    \n",
    "    Writes vocabulary line by line to `preprocessed/fname`\n",
    "    '''  \n",
    "    text = regex.sub(\"[^\\s\\p{Latin}']\", \"\", text)\n",
    "    words = text.split()\n",
    "    word2cnt = Counter(words)\n",
    "    if not os.path.exists('./data/preprocessed'): \n",
    "        os.mkdir('./data/preprocessed')\n",
    "    with codecs.open('./data/preprocessed/{}'.format(fname), 'w', 'utf-8') as fout:\n",
    "        fout.write(\"{}\\t100000\\n{}\\t100000\\n{}\\t100000\\n{}\\t100000\\n\".format(\"<PAD>\", \"<UNK>\", \"<S>\", \"</S>\"))\n",
    "        for word, cnt in word2cnt.most_common(len(word2cnt)):\n",
    "            fout.write(u\"{}\\t{}\\n\".format(word, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('./data/preprocessed')\n",
    "\n",
    "# create vocab lists from our text files\n",
    "make_vocab(fname=\"oldbible.tsv\", text=bibles.literal_bib.to_string().lower())\n",
    "make_vocab(fname=\"newbible.tsv\", text=bibles.new_bib.to_string().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#/usr/bin/python2\n",
    "'''\n",
    "June 2017 by kyubyong park. \n",
    "kbpark.linguist@gmail.com.\n",
    "https://www.github.com/kyubyong/transformer\n",
    "'''\n",
    "import numpy as np\n",
    "import codecs\n",
    "import regex\n",
    "\n",
    "# load source vocab\n",
    "def make_embed(word_dict):\n",
    "    EMBEDDING_FILE =  './data/fasttext-wikinews/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr[:-1], dtype='float32')\n",
    "#     embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(EMBEDDING_FILE, encoding = \"utf8\"))) \n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    for i, word in enumerate(word_dict):\n",
    "        embedding_vector = embeddings_index.get(word, None)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "        \n",
    "def load_de_vocab(vocab_only = False):\n",
    "    vocab = [line.split()[0] for line in codecs.open('./data/preprocessed/oldbible.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])>=hp.min_cnt]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    if vocab_only == True:\n",
    "        return word2idx, idx2word\n",
    "    embedding_matrix = make_embed(vocab)\n",
    "        \n",
    "    return word2idx, idx2word, embedding_matrix\n",
    "\n",
    "# load target vocab\n",
    "def load_en_vocab(vocab_only = False):\n",
    "    vocab = [line.split()[0] for line in codecs.open('./data/preprocessed/newbible.tsv', 'r', 'utf-8').read().splitlines() if int(line.split()[1])>=hp.min_cnt]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    if vocab_only == True:\n",
    "        return word2idx, idx2word\n",
    "    embedding_matrix = make_embed(vocab)\n",
    "    return word2idx, idx2word, embedding_matrix\n",
    "\n",
    "def create_data(source_sents, target_sents):\n",
    "    print(len(source_sents))\n",
    "    de2idx, idx2de = load_de_vocab(vocab_only = True)\n",
    "    en2idx, idx2en = load_en_vocab(vocab_only = True)\n",
    "    \n",
    "    # Index\n",
    "    x_list, y_list, Sources, Targets = [], [], [], []\n",
    "    for source_sent, target_sent in zip(source_sents, target_sents):\n",
    "        x = [de2idx.get(word, 1) for word in (source_sent + u\" </S>\").split()] # 1: OOV, </S>: End of Text\n",
    "        y = [en2idx.get(word, 1) for word in (target_sent + u\" </S>\").split()] \n",
    "        if max(len(x), len(y)) <=hp.maxlen:\n",
    "            x_list.append(np.array(x))\n",
    "            y_list.append(np.array(y))\n",
    "            Sources.append(source_sent)\n",
    "            Targets.append(target_sent)\n",
    "    # Pad   \n",
    "    X = np.zeros([len(x_list), hp.maxlen], np.int32)\n",
    "    Y = np.zeros([len(y_list), hp.maxlen], np.int32)\n",
    "    for i, (x, y) in enumerate(zip(x_list, y_list)):\n",
    "        X[i] = np.lib.pad(x, [0, hp.maxlen-len(x)], 'constant', constant_values=(0, 0))\n",
    "        Y[i] = np.lib.pad(y, [0, hp.maxlen-len(y)], 'constant', constant_values=(0, 0))\n",
    "    \n",
    "    return X, Y, Sources, Targets\n",
    "\n",
    "def load_train_data():\n",
    "    de_sents = hp.source_train\n",
    "    en_sents = hp.target_train\n",
    "    \n",
    "    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n",
    "    return X, Y\n",
    "\n",
    "def load_val_data():\n",
    "    de_sents = hp.source_val\n",
    "    en_sents = hp.target_val\n",
    "    \n",
    "    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n",
    "    return X, Y\n",
    "    \n",
    "def load_test_data():\n",
    "    def _refine(line):\n",
    "        line = regex.sub(\"<[^>]+>\", \"\", line)\n",
    "        line = regex.sub(\"[^\\s\\p{Latin}']\", \"\", line) \n",
    "        return line.strip()\n",
    "    \n",
    "    de_sents = hp.source_test\n",
    "    en_sents = hp.target_test\n",
    "    \n",
    "    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n",
    "    return X, Sources, Targets # (1064, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000001it [01:17, 12824.80it/s]\n",
      "1000001it [01:18, 12806.73it/s]\n"
     ]
    }
   ],
   "source": [
    "hp = Hyperparams\n",
    "de2idx, idx2de, embedding_matrix1 = load_de_vocab()\n",
    "en2idx, idx2en, embedding_matrix2 = load_en_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "               decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                            src_mask, trg_mask, hidden=decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # we need to manually concatenate the final states for both directions\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
    "\n",
    "        return output, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                          hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "\n",
    "        # compute context vector using attention mechanism\n",
    "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
    "        context, attn_probs = self.attention(\n",
    "            query=query, proj_key=proj_key,\n",
    "            value=encoder_hidden, mask=src_mask)\n",
    "\n",
    "        # update rnn hidden state\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "\n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
    "                src_mask, trg_mask, hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "                                         \n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "        \n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "        \n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "        \n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        # We first project the query (the decoder state).\n",
    "        # The projected keys (the encoder states) were already pre-computated.\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate scores.\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Turn scores to probabilities.\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas        \n",
    "        \n",
    "        # The context vector is the weighted sum of the values.\n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
    "        return context, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size, tgt_vocab))\n",
    "\n",
    "    return model.cuda() if USE_CUDA else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\n",
    "    Input is a batch from a torch text iterator.\n",
    "    \"\"\"\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        \n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \"\"\"Standard Training and Logging Function\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                           batch.src_mask, batch.trg_mask,\n",
    "                                           batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        if model.training and i % print_every == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
    "            start = time.time()\n",
    "            print_tokens = 0\n",
    "\n",
    "    return math.exp(total_loss / float(total_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(example_iter, model, n=2, max_len=100, \n",
    "                   sos_index=1, \n",
    "                   src_eos_index=None, \n",
    "                   trg_eos_index=None, \n",
    "                   src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None:\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "        \n",
    "    for i, batch in enumerate(example_iter):\n",
    "        if(i % 2 == 0):\n",
    "            continue\n",
    "        src = batch.src.cpu().numpy()[0, :]\n",
    "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
    "\n",
    "        # remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
    "      \n",
    "        result, _ = greedy_decode(\n",
    "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
    "        print(\"Example #%d\" % (count+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(num_words=11, batch_size=16, num_batches=100, length=10, pad_index=0, sos_index=1):\n",
    "    \"\"\"Generate random data for a src-tgt copy task.\"\"\"\n",
    "    for i in range(num_batches):\n",
    "        data = torch.from_numpy(\n",
    "          np.random.randint(1, num_words, size=(batch_size, length)))\n",
    "        data[:, 0] = sos_index\n",
    "        data = data.cuda() if USE_CUDA else data\n",
    "        src = data[:, 1:]\n",
    "        trg = data\n",
    "        src_lengths = [length-1] * batch_size\n",
    "        trg_lengths = [length] * batch_size\n",
    "        yield Batch((src, src_lengths), (trg, trg_lengths), pad_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "\n",
    "        if self.opt is not None:\n",
    "            loss.backward()          \n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \"\"\"Greedily decode a sentence.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "\n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(\n",
    "              encoder_hidden, encoder_final, src_mask,\n",
    "              prev_y, trg_mask, hidden)\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "    \n",
    "    output = np.array(output)\n",
    "        \n",
    "    # cut off everything starting from </s> \n",
    "    # (only when eos_index provided)\n",
    "    if eos_index is not None:\n",
    "        first_eos = np.where(output==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]      \n",
    "    \n",
    "    return output, np.concatenate(attention_scores, axis=1)\n",
    "  \n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "\n",
    "    return [str(t) for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import en_core_web_sm\n",
    "en = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"    \n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "LOWER = True\n",
    "\n",
    "# we include lengths to provide to the RNNs\n",
    "SRC = data.Field(tokenize=tokenize_en, \n",
    "                 batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                 unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
    "\n",
    "TRG = data.Field(tokenize=tokenize_en, \n",
    "                 batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                 unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "\n",
    "MAX_LEN_SRC = 50 \n",
    "MAX_LEN_TRG = 50  \n",
    "\n",
    "fields = [\n",
    "    ('ID', None), # we dont need this, so no processing\n",
    "    ('src', SRC), # process it as label\n",
    "    ('trg', TRG) # process it as text\n",
    "]\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(path='./data/preprocessed/',train='train_df.csv',\n",
    "                                    validation='val_df.csv', test='test_df.csv', format='csv',\n",
    "                                    fields=fields,\n",
    "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN_SRC \n",
    "                                    and len(vars(x)['trg']) <= MAX_LEN_TRG)\n",
    "\n",
    "MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
    "\n",
    "SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
    "PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes (number of sentence pairs):\n",
      "train 19712\n",
      "valid 5624\n",
      "test 2808 \n",
      "\n",
      "First training example:\n",
      "src: trg\n",
      "trg: src \n",
      "\n",
      "Most common words (src):\n",
      "         ,      52595\n",
      "       and      33284\n",
      "       the      31392\n",
      "        of      20885\n",
      "        to      12253\n",
      "         .      10754\n",
      "         `      10136\n",
      "         '       8896\n",
      "        in       8426\n",
      "        is       7120 \n",
      "\n",
      "Most common words (trg):\n",
      "         ,      40917\n",
      "       the      32855\n",
      "         .      19527\n",
      "       and      19523\n",
      "        of      19282\n",
      "        to      11912\n",
      "       you       8795\n",
      "        in       7555\n",
      "        he       6058\n",
      "         ;       5787 \n",
      "\n",
      "First 10 words (src):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 </s>\n",
      "03 ,\n",
      "04 and\n",
      "05 the\n",
      "06 of\n",
      "07 to\n",
      "08 .\n",
      "09 ` \n",
      "\n",
      "First 10 words (trg):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 ,\n",
      "05 the\n",
      "06 .\n",
      "07 and\n",
      "08 of\n",
      "09 to \n",
      "\n",
      "Number of text words (types): 4129\n",
      "Number of title words (types): 4104 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    print(\"Data set sizes (number of sentence pairs):\")\n",
    "    print('train', len(train_data))\n",
    "    print('valid', len(valid_data))\n",
    "    print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
    "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
    "    \n",
    "    print(\"Most common words (src):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    print(\"Most common words (trg):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    print(\"First 10 words (src):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    print(\"First 10 words (trg):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of text words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of title words (types):\", len(trg_field.vocab), \"\\n\")\n",
    "    \n",
    "    \n",
    "print_data_info(train_data, valid_data, test_data,   SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
